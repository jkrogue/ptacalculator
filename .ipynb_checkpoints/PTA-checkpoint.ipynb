{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_excel(\"PTA.xlsx\")\n",
    "pd.set_option(\"max_columns\", 9999)\n",
    "rand_state=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the 1 missing age row:\n",
    "\n",
    "data['Age'] = data['Age'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert continuous age variable to categorical:\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    if row['Age'] < 18:\n",
    "        data.loc[idx, 'Age_cat'] = \"Teenager\"\n",
    "    elif row['Age'] >= 18 and row['Age'] < 30:\n",
    "        data.loc[idx, 'Age_cat'] = \"Young Adult\"\n",
    "    elif row['Age'] >= 30 and row['Age'] < 50:\n",
    "        data.loc[idx, 'Age_cat'] = \"Adult\"\n",
    "    elif row['Age'] >50:\n",
    "        data.loc[idx, 'Age_cat'] = \"50+\"\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    \n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df\n",
    "\n",
    "data = create_dummies(data, 'Age_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert gender to numerical:\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['Gender'] == \"M\":\n",
    "        data.loc[index,'Gender'] = 1\n",
    "    else:\n",
    "        data.loc[index,'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NR (null) value from Duration of symptoms:\n",
    "\n",
    "data = data [data['Duration Sxs (days)']!= \"NR\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Duration Sxs (days)</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Sore Throat</th>\n",
       "      <th>Worsening of Symptoms</th>\n",
       "      <th>Otalgia</th>\n",
       "      <th>Trismus</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Voice Change</th>\n",
       "      <th>Dysphagia</th>\n",
       "      <th>Dyspnea</th>\n",
       "      <th>Anorexia</th>\n",
       "      <th>Neck Pain</th>\n",
       "      <th>LND</th>\n",
       "      <th>Other Sxs</th>\n",
       "      <th>Previously Tx</th>\n",
       "      <th>Recurrent Event</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Antibx</th>\n",
       "      <th>Aspiration Att</th>\n",
       "      <th>Pus</th>\n",
       "      <th>Amount Pus</th>\n",
       "      <th>Admit Days</th>\n",
       "      <th>Quinsy Tonsillectomy</th>\n",
       "      <th>Tonsillectomy</th>\n",
       "      <th>Tonsillectomy Date</th>\n",
       "      <th>Interval to Tonsillectomy</th>\n",
       "      <th>Tonsil Bleed</th>\n",
       "      <th>Tonsil Bleed Date</th>\n",
       "      <th>Tonsil Bleed Tx Method</th>\n",
       "      <th>Strep</th>\n",
       "      <th>Mono</th>\n",
       "      <th>CT</th>\n",
       "      <th>F/u</th>\n",
       "      <th>Recurrence</th>\n",
       "      <th>Recurrence Date #1</th>\n",
       "      <th>Interval to Rec #1 (days)</th>\n",
       "      <th>Recurrence Date #2</th>\n",
       "      <th>Recurrence Date #3</th>\n",
       "      <th>F. nucleatum</th>\n",
       "      <th>F. Necrophorum</th>\n",
       "      <th>Strep species</th>\n",
       "      <th>Staph species</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Eikenella</th>\n",
       "      <th>H. flu</th>\n",
       "      <th>Other</th>\n",
       "      <th>Oral Flora</th>\n",
       "      <th>No organisms grown</th>\n",
       "      <th>None Sent</th>\n",
       "      <th>Culture Data</th>\n",
       "      <th>ITA</th>\n",
       "      <th>ITA-R</th>\n",
       "      <th>ITA-C</th>\n",
       "      <th>PTA</th>\n",
       "      <th>Other Dx</th>\n",
       "      <th>Other Notes</th>\n",
       "      <th>Unnamed: 59</th>\n",
       "      <th>Age_cat</th>\n",
       "      <th>Age_cat_50+</th>\n",
       "      <th>Age_cat_Adult</th>\n",
       "      <th>Age_cat_Teenager</th>\n",
       "      <th>Age_cat_Young Adult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fatigue</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exudative tonsillitis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Young Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not called per radiologist, but seen on my int...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teenager</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fusobacterium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teenager</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None sent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Young Adult</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender Duration Sxs (days)  Fever  Sore Throat  \\\n",
       "0  55.0       1                   4    0.0          1.0   \n",
       "1  19.0       1                   4    1.0          1.0   \n",
       "2  14.0       0                   6    0.0          1.0   \n",
       "3  10.0       0                   7    0.0          1.0   \n",
       "4  28.0       0                   5    0.0          1.0   \n",
       "\n",
       "   Worsening of Symptoms  Otalgia  Trismus  Cough  Voice Change  Dysphagia  \\\n",
       "0                    1.0      0.0      1.0    0.0           1.0        0.0   \n",
       "1                    0.0      0.0      0.0    0.0           0.0        1.0   \n",
       "2                    0.0      0.0      0.0    0.0           1.0        0.0   \n",
       "3                    1.0      1.0      1.0    0.0           1.0        0.0   \n",
       "4                    1.0      1.0      1.0    0.0           1.0        0.0   \n",
       "\n",
       "   Dyspnea  Anorexia  Neck Pain LND Other Sxs  Previously Tx  Recurrent Event  \\\n",
       "0      0.0       0.0        0.0   0         0            4.0              1.0   \n",
       "1      0.0       1.0        0.0   0   Fatigue            1.0              NaN   \n",
       "2      0.0       1.0        0.0   1       NaN            0.0              NaN   \n",
       "3      0.0       0.0        0.0   0       NaN            2.0              NaN   \n",
       "4      0.0       1.0        0.0   1       NaN            4.0              1.0   \n",
       "\n",
       "   WBC  Antibx  Aspiration Att  Pus Amount Pus  Admit Days  \\\n",
       "0  14.1      1             1.0  1.0          3         2.0   \n",
       "1     6      1             0.0  NaN        NaN         0.0   \n",
       "2   NaN      1             0.0  NaN        NaN         0.0   \n",
       "3   NaN      1             0.0  NaN        NaN         0.0   \n",
       "4  12.7      1             1.0  1.0         NR         0.0   \n",
       "\n",
       "   Quinsy Tonsillectomy Tonsillectomy Tonsillectomy Date  \\\n",
       "0                   NaN             0                NaT   \n",
       "1                   NaN             0                NaT   \n",
       "2                   NaN             0                NaT   \n",
       "3                   1.0             1         2013-12-05   \n",
       "4                   NaN             0                NaT   \n",
       "\n",
       "   Interval to Tonsillectomy  Tonsil Bleed Tonsil Bleed Date  \\\n",
       "0                        NaN           NaN               NaN   \n",
       "1                        NaN           NaN               NaN   \n",
       "2                        NaN           NaN               NaN   \n",
       "3                        NaN           0.0               NaN   \n",
       "4                        NaN           NaN               NaN   \n",
       "\n",
       "   Tonsil Bleed Tx Method Strep Mono   CT  F/u  Recurrence Recurrence Date #1  \\\n",
       "0                     NaN   NaN  NaN  0.0  0.0         0.0                NaT   \n",
       "1                     NaN   NaN    0  1.0  0.0         0.0                NaT   \n",
       "2                     NaN     1    0  1.0  0.0         0.0                NaT   \n",
       "3                     NaN   NaN  NaN  1.0  0.0         0.0                NaT   \n",
       "4                     NaN     0  NaN  1.0  0.0         0.0                NaT   \n",
       "\n",
       "   Interval to Rec #1 (days) Recurrence Date #2 Recurrence Date #3  \\\n",
       "0                        NaN                NaT                NaT   \n",
       "1                        NaN                NaT                NaT   \n",
       "2                        NaN                NaT                NaT   \n",
       "3                        NaN                NaT                NaT   \n",
       "4                        NaN                NaT                NaT   \n",
       "\n",
       "   F. nucleatum  F. Necrophorum  Strep species  Staph species  Prevotella  \\\n",
       "0           NaN             NaN            NaN            NaN         NaN   \n",
       "1           NaN             NaN            NaN            NaN         NaN   \n",
       "2           NaN             NaN            NaN            NaN         NaN   \n",
       "3           1.0             NaN            NaN            NaN         NaN   \n",
       "4           NaN             NaN            NaN            NaN         NaN   \n",
       "\n",
       "   Bacteroides  Eikenella  H. flu Other  Oral Flora  No organisms grown  \\\n",
       "0          NaN        NaN     NaN   NaN         NaN                 1.0   \n",
       "1          NaN        NaN     NaN   NaN         NaN                 NaN   \n",
       "2          NaN        NaN     NaN   NaN         NaN                 NaN   \n",
       "3          NaN        NaN     NaN   NaN         NaN                 NaN   \n",
       "4          NaN        NaN     NaN   NaN         NaN                 NaN   \n",
       "\n",
       "   None Sent    Culture Data  ITA  ITA-R  ITA-C  PTA               Other Dx  \\\n",
       "0        NaN               0  NaN    NaN    NaN  1.0                    NaN   \n",
       "1        NaN             NaN  NaN    NaN    NaN  0.0  Exudative tonsillitis   \n",
       "2        NaN             NaN  1.0    1.0    0.0  0.0                    NaN   \n",
       "3        NaN  Fusobacterium   NaN    NaN    NaN  1.0                    NaN   \n",
       "4        1.0       None sent  NaN    NaN    NaN  1.0                    NaN   \n",
       "\n",
       "                                         Other Notes  Unnamed: 59  \\\n",
       "0                                                NaN          NaN   \n",
       "1                                                NaN          NaN   \n",
       "2  Not called per radiologist, but seen on my int...          NaN   \n",
       "3                                                NaN          NaN   \n",
       "4                                                NaN          NaN   \n",
       "\n",
       "       Age_cat  Age_cat_50+  Age_cat_Adult  Age_cat_Teenager  \\\n",
       "0          50+            1              0                 0   \n",
       "1  Young Adult            0              0                 0   \n",
       "2     Teenager            0              0                 1   \n",
       "3     Teenager            0              0                 1   \n",
       "4  Young Adult            0              0                 0   \n",
       "\n",
       "   Age_cat_Young Adult  \n",
       "0                    0  \n",
       "1                    1  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    1  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deal with missing values for WBC count:\n",
    "\n",
    "import numpy as np\n",
    "for idx,row in data.iterrows():\n",
    "    if row['WBC '] == 'Not Performed':\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "    elif row['WBC '] == 0:\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "        \n",
    "data['WBC '] = data['WBC '].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Previously Tx to categotical columnns:\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    if row['Previously Tx'] == 1:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Antibiotics Alone\"\n",
    "    elif row['Previously Tx'] == 2:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Steroids Alone\"\n",
    "    elif row['Previously Tx'] == 3:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Abx Steroids\"\n",
    "    elif row['Previously Tx'] == 4:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Abx + Aspiration attempt\"\n",
    "    elif row['Previously Tx'] == 5:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Pain Meds alone\"\n",
    "    elif row['Previously Tx'] == 0:\n",
    "        data.loc[idx, 'Previously Tx'] = \"no treatment\"\n",
    "        \n",
    "data = create_dummies(data, 'Previously Tx').drop(columns='Previously Tx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WBC was excluded because it is weakly correlated and not always available pre-aspiration attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 914 entries, 0 to 1056\n",
      "Data columns (total 8 columns):\n",
      "Age                           914 non-null float64\n",
      "Duration Sxs (days)           914 non-null float64\n",
      "Otalgia                       914 non-null float64\n",
      "Trismus                       914 non-null float64\n",
      "Worsening of Symptoms         914 non-null float64\n",
      "Pus                           914 non-null float64\n",
      "Previously Tx_no treatment    914 non-null uint8\n",
      "Neck Pain                     914 non-null float64\n",
      "dtypes: float64(7), uint8(1)\n",
      "memory usage: 58.0 KB\n"
     ]
    }
   ],
   "source": [
    "#convert to numeric, drop rows with missing data\n",
    "data['Duration Sxs (days)'] = data['Duration Sxs (days)'].astype(\"float64\")\n",
    "all_data = data #preserve all columns for descriptive stats later\n",
    "data = data[['Age','Duration Sxs (days)', 'Otalgia', \n",
    "           'Trismus','Worsening of Symptoms', 'Pus', 'Previously Tx_no treatment', 'Neck Pain']]\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "914 rows remain after dropping all with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pus                                       1.000000\n",
       "Trismus                                   0.402624\n",
       "Worsening of Symptoms                     0.261131\n",
       "Duration Sxs (days)                       0.122879\n",
       "Otalgia                                   0.080643\n",
       "Previously Tx_Steroids Alone              0.077171\n",
       "Dysphagia                                 0.076120\n",
       "Previously Tx_Abx Steroids                0.056453\n",
       "Age_cat_Young Adult                       0.052571\n",
       "Gender                                    0.050748\n",
       "Previously Tx_Abx + Aspiration attempt    0.045662\n",
       "Previously Tx_6.0                         0.036192\n",
       "Anorexia                                  0.019250\n",
       "Age                                       0.019019\n",
       "Previously Tx_Pain Meds alone             0.006112\n",
       "Age_cat_50+                               0.001083\n",
       "Age_cat_Adult                            -0.014408\n",
       "Fever                                    -0.025973\n",
       "Age_cat_Teenager                         -0.050687\n",
       "Cough                                    -0.060276\n",
       "Previously Tx_no treatment               -0.093616\n",
       "Neck Pain                                -0.095822\n",
       "Name: Pus, dtype: float64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = all_data[['Age','Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult', 'Pus', 'Previously Tx_no treatment', \n",
    "             'Previously Tx_Steroids Alone', 'Previously Tx_Pain Meds alone', 'Previously Tx_Abx Steroids',\n",
    "             'Previously Tx_Abx + Aspiration attempt', 'Previously Tx_6.0', 'Neck Pain']].corr()\n",
    "corr['Pus'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean age = 24.15469194312796, (1.0-88.0)\n",
      "Percentage male = 0.5\n",
      "\n",
      "percent successful aspiration: 0.5426136363636364\n"
     ]
    }
   ],
   "source": [
    "print('mean age = {}, ({}-{})'.format(np.mean(all_data['Age']), all_data['Age'].min(), all_data['Age'].max()))\n",
    "print(\"Percentage male = {}\".format(all_data['Gender'].sum()/all_data['Gender'].shape[0]))\n",
    "print(\"\")\n",
    "print(\"percent successful aspiration: {}\".format(all_data['Pus'].sum()/all_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Anorexia</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Duration Sxs (days)</th>\n",
       "      <th>Dysphagia</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Neck Pain</th>\n",
       "      <th>Otalgia</th>\n",
       "      <th>Previously Tx_no treatment</th>\n",
       "      <th>Trismus</th>\n",
       "      <th>Worsening of Symptoms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>24.370845</td>\n",
       "      <td>0.463557</td>\n",
       "      <td>0.067647</td>\n",
       "      <td>5.868805</td>\n",
       "      <td>0.594752</td>\n",
       "      <td>0.452941</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.254386</td>\n",
       "      <td>0.539359</td>\n",
       "      <td>0.562682</td>\n",
       "      <td>0.336257</td>\n",
       "      <td>0.594752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>24.825480</td>\n",
       "      <td>0.483421</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>7.136126</td>\n",
       "      <td>0.670157</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.521815</td>\n",
       "      <td>0.174520</td>\n",
       "      <td>0.621291</td>\n",
       "      <td>0.465969</td>\n",
       "      <td>0.745201</td>\n",
       "      <td>0.830716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  Anorexia     Cough  Duration Sxs (days)  Dysphagia     Fever  \\\n",
       "Pus                                                                            \n",
       "0.0  24.370845  0.463557  0.067647             5.868805   0.594752  0.452941   \n",
       "1.0  24.825480  0.483421  0.040351             7.136126   0.670157  0.426316   \n",
       "\n",
       "       Gender  Neck Pain   Otalgia  Previously Tx_no treatment   Trismus  \\\n",
       "Pus                                                                        \n",
       "0.0  0.469388   0.254386  0.539359                    0.562682  0.336257   \n",
       "1.0  0.521815   0.174520  0.621291                    0.465969  0.745201   \n",
       "\n",
       "     Worsening of Symptoms  \n",
       "Pus                         \n",
       "0.0               0.594752  \n",
       "1.0               0.830716  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(all_data, index=['Pus'],  values = ['Trismus', 'Worsening of Symptoms', 'Duration Sxs (days)', \n",
    "                                               'Otalgia', 'Dysphagia', 'Gender', 'Anorexia', 'Age', 'Fever',\n",
    "                                               'Cough', 'Neck Pain', 'Previously Tx_no treatment' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trismus looks much more predictive of successful aspiration than otalgia (Trismus means pain with opening your mouth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for storing performance measures for comparison between models\n",
    "accuracy_dict = {}\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# we include the 6 features most correlated with the presence of pus.\n",
    "features = ['Duration Sxs (days)', 'Otalgia', \n",
    "           'Trismus','Worsening of Symptoms', \n",
    "            'Neck Pain', 'Previously Tx_no treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and holdout sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 5}\n",
      "0.710031347962\n"
     ]
    }
   ],
   "source": [
    "# Grid search for parameter optimization\n",
    "# this may take a minute to run...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier()\n",
    "grid = GridSearchCV(rf, {\n",
    "            \"n_estimators\": [5, 10, 100],\n",
    "            \"criterion\": ['gini'],\n",
    "            \"max_depth\": [1, 3, 5, 7, 9, 15],\n",
    "            \"max_features\": [\"log2\"],\n",
    "            \"min_samples_leaf\": [1, 3, 5, 8, 10],\n",
    "            \"min_samples_split\": [2, 3, 5, 8, 10],\n",
    "            'random_state': [rand_state],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']\n",
    "        }, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7038077858880778(0.02520242930351566) \n",
      " Sensitivity: 0.7396584967397752(0.04285446615071152) \n",
      " Specificity: 0.6436898247896068 (0.05191677132418514) \n",
      " NPV: 0.5958362944329777 (0.045075448667591195)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_accuracy = []\n",
    "rf_accuracy_train = []\n",
    "rf_ppv = []\n",
    "rf_npv = []\n",
    "rf_sensitivity = []\n",
    "rf_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_rf.fit(X_train, y_train)\n",
    "    predictions = best_rf.predict(X_test)\n",
    "    predictions_train = best_rf.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    rf_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    rf_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    rf_ppv.append(ppv)\n",
    "    rf_npv.append(npv)\n",
    "    rf_sensitivity.append(sensitivity)\n",
    "    rf_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['Random Forest'] = np.mean(rf_accuracy)\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({}) \\n NPV: {} ({})\".format(np.mean(rf_accuracy), \n",
    "                                                                               np.std(rf_accuracy), \n",
    "                                                                               np.mean(rf_sensitivity),\n",
    "                                                                               np.std(rf_sensitivity),\n",
    "                                                                               np.mean(rf_specificity),\n",
    "                                                                              np.std(rf_specificity), np.mean(rf_npv),\n",
    "                                                                                              np.std(rf_npv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699059561129\n",
      "{'class_weight': 'balanced', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(lr, {\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"], 'class_weight': ['', 'balanced']}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_lr = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726277372263 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.67      0.66       110\n",
      "        1.0       0.78      0.76      0.77       164\n",
      "\n",
      "avg / total       0.73      0.73      0.73       274\n",
      "\n",
      "0.717461197339\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of LR\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "best_lr.fit(X_train, y_train)\n",
    "predictions = best_lr.predict(X_test)\n",
    "predictions_train = best_lr.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_test, predictions), \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.780787237944 0.619293454297\n",
      "Accuracy: 0.7185158150851582(0.023301699479211232) \n",
      " Sensitivity: 0.7677121375503677(0.03483854278310652) \n",
      " Specificity: 0.6356344748492795 (0.05290724887382551) \n",
      " NPV: 0.6192934542971635 (0.04127430421532957)\n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = []\n",
    "lr_accuracy_train = []\n",
    "lr_ppv = []\n",
    "lr_npv = []\n",
    "lr_sensitivity = []\n",
    "lr_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_lr.fit(X_train, y_train)\n",
    "    predictions = best_lr.predict(X_test)\n",
    "    predictions_train = best_lr.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    lr_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    lr_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    lr_ppv.append(ppv)\n",
    "    lr_npv.append(npv)\n",
    "    lr_sensitivity.append(sensitivity)\n",
    "    lr_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['Log regression'] = np.mean(lr_accuracy)\n",
    "print(np.mean(lr_ppv), np.mean(lr_npv))    \n",
    "    \n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({}) \\n NPV: {} ({})\".format(np.mean(lr_accuracy), \n",
    "                                                                               np.std(lr_accuracy), \n",
    "                                                                               np.mean(lr_sensitivity),\n",
    "                                                                               np.std(lr_sensitivity),\n",
    "                                                                               np.mean(lr_specificity),\n",
    "                                                                              np.std(lr_specificity), np.mean(lr_npv),\n",
    "                                                                                              np.std(lr_npv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is less accurate than Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733542319749\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 13, 'p': 1, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_knn = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7173844282238443(0.021186539168266145) \n",
      " Sensitivity: 0.8724708864532681(0.028565775313766004) \n",
      " Specificity: 0.45728235725730143 (0.0517754898311601) \n",
      " NPV: 0.6814549497961365 (0.05339267402654493)\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = []\n",
    "knn_accuracy_train = []\n",
    "knn_ppv = []\n",
    "knn_npv = []\n",
    "knn_sensitivity = []\n",
    "knn_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_knn.fit(X_train, y_train)\n",
    "    predictions = best_knn.predict(X_test)\n",
    "    predictions_train = best_knn.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    knn_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    knn_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    knn_ppv.append(ppv)\n",
    "    knn_npv.append(npv)\n",
    "    knn_sensitivity.append(sensitivity)\n",
    "    knn_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['KNN'] = np.mean(knn_accuracy)    \n",
    "    \n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({}) \\n NPV: {} ({})\".format(np.mean(knn_accuracy), \n",
    "                                                                               np.std(knn_accuracy), \n",
    "                                                                               np.mean(knn_sensitivity),\n",
    "                                                                               np.std(knn_sensitivity),\n",
    "                                                                               np.mean(knn_specificity),\n",
    "                                                                              np.std(knn_specificity), \n",
    "                                                                               np.mean(knn_npv),\n",
    "                                                                              np.std(knn_npv)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715328467153 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.47      0.57       110\n",
      "        1.0       0.71      0.88      0.79       164\n",
      "\n",
      "avg / total       0.72      0.72      0.70       274\n",
      "\n",
      "0.675388026608\n"
     ]
    }
   ],
   "source": [
    "# evaluate knn performance:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "best_knn.fit(X_train, y_train)\n",
    "predictions = best_knn.predict(X_test)\n",
    "predictions_train = best_knn.predict(X_train)\n",
    "    \n",
    "print(accuracy_score(y_test, predictions) , \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729927007299 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.54      0.61       110\n",
      "        1.0       0.73      0.86      0.79       164\n",
      "\n",
      "avg / total       0.73      0.73      0.72       274\n",
      "\n",
      "0.698059866962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000, random_state=rand_state)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy , \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728840125392\n",
      "{'alpha': 1, 'hidden_layer_sizes': [300, 100, 50], 'max_iter': 2000, 'random_state': 5}\n"
     ]
    }
   ],
   "source": [
    "#evaluate optimal parameters:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier()\n",
    "grid = GridSearchCV(mlp, {\n",
    "            'alpha': [10, 1, .01, .001],\n",
    "            'max_iter': [2000],\n",
    "            'hidden_layer_sizes': [[300, 150], [300, 100, 50]],\n",
    "            'random_state':[rand_state]}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_mlp = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711678832117              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.52      0.59       110\n",
      "        1.0       0.72      0.84      0.78       164\n",
      "\n",
      "avg / total       0.71      0.71      0.70       274\n",
      "\n",
      "(array([ 0.68674699,  0.72251309]), array([ 0.51818182,  0.84146341]), array([ 0.59067358,  0.77746479]), array([110, 164]))\n"
     ]
    }
   ],
   "source": [
    "#evaluate performance:\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "predictions = best_mlp.predict(X_test)\n",
    "predictions_train = best_mlp.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_test, predictions), classification_report(y_test, predictions))\n",
    "print(precision_recall_fscore_support(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7264355231143552(0.022963172506737125) \n",
      " Sensitivity: 0.8532281611089727(0.04151172003150451) \n",
      " Specificity: 0.5144280010759564 (0.06453334971311799) \n",
      " NPV: 0.6789035874326388 (0.05886908546774301) \n",
      " PPV: 0.7481121045837162 (0.031050470467013657)\n"
     ]
    }
   ],
   "source": [
    "mlp_accuracy = []\n",
    "mlp_accuracy_train = []\n",
    "mlp_ppv = []\n",
    "mlp_npv = []\n",
    "mlp_sensitivity = []\n",
    "mlp_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "    predictions = best_mlp.predict(X_test)\n",
    "    predictions_train = best_mlp.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    mlp_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    mlp_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    mlp_ppv.append(ppv)\n",
    "    mlp_npv.append(npv)\n",
    "    mlp_sensitivity.append(sensitivity)\n",
    "    mlp_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['MLP'] = np.mean(mlp_accuracy)\n",
    "\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({}) \\n NPV: {} ({}) \\n PPV: {} ({})\".format(np.mean(mlp_accuracy), \n",
    "                                                                               np.std(mlp_accuracy), \n",
    "                                                                               np.mean(mlp_sensitivity),\n",
    "                                                                               np.std(mlp_sensitivity),\n",
    "                                                                               np.mean(mlp_specificity),\n",
    "                                                                              np.std(mlp_specificity), \n",
    "                                                                                   np.mean(mlp_npv),\n",
    "                                                                                  np.std(mlp_npv), \n",
    "                                                                                np.mean(mlp_ppv), \n",
    "                                                                              np.std(mlp_ppv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superior to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7098661800486618(0.021827743404467904) \n",
      " Sensitivity: 0.8224211160104723(0.035235617104318315) \n",
      " Specificity: 0.521606929383016 (0.05859041260529382)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_accuracy = []\n",
    "gbc_accuracy_train = []\n",
    "gbc_ppv = []\n",
    "gbc_npv = []\n",
    "gbc_sensitivity = []\n",
    "gbc_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    predictions = gbc.predict(X_test)\n",
    "    predictions_train = gbc.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    gbc_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    gbc_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    gbc_ppv.append(ppv)\n",
    "    gbc_npv.append(npv)\n",
    "    gbc_sensitivity.append(sensitivity)\n",
    "    gbc_specificity.append(specificity)    \n",
    "\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(gbc_accuracy), \n",
    "                                                                               np.std(gbc_accuracy), \n",
    "                                                                               np.mean(gbc_sensitivity),\n",
    "                                                                               np.std(gbc_sensitivity),\n",
    "                                                                               np.mean(gbc_specificity),\n",
    "                                                                              np.std(gbc_specificity)))\n",
    "accuracy_dict['SVM'] = np.mean(gbc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Random Forest': 0.69782238442822386, 'Log regression': 0.72655717761557181, 'KNN': 0.70596107055961077, 'MLP': 0.68978102189781021, 'SVM': 0.70986618004866175}\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, ENT specialists are only 64% accurate, so this is better than human prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is leftover code from other explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.716300940439\n",
      "{'C': 100, 'class_weight': 'balanced', 'shrinking': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "grid = GridSearchCV(svc, {\n",
    "            'C': [100, 10, 1, .01, .001, .0001],\n",
    "            'shrinking': [True, False],\n",
    "            'class_weight':['balanced','']}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_svc = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701605839416 0.706238244514\n",
      "0.695301640256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "svc_accuracy = []\n",
    "svc_accuracy_train = []\n",
    "for i in range (100):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.3, random_state=i)\n",
    "\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    best_svc.fit(X_train, y_train)\n",
    "    predictions = best_svc.predict(X_test)\n",
    "    predictions_train = best_svc.predict(X_train)\n",
    "    svc_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    svc_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "\n",
    "print(np.mean(svc_accuracy), np.mean(svc_accuracy_train))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mlp_final = MLPClassifier(alpha=1, hidden_layer_sizes=[300, 100, 50], max_iter=2000, random_state=5)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data[features])\n",
    "mlp_final.fit(data[features], data['Pus'])\n",
    "sc_data = scaler.transform(data[features])\n",
    "\n",
    "pipe = Pipeline([('scaler',scaler), ('classifier',mlp_final)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.pkl']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipe, 'final_model.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e6b3651a636f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mbest_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_selector(df, features):\n",
    "    \n",
    "    models = [{\n",
    "        \"name\": 'K Neighbors Classifier', \n",
    "        'estimator':KNeighborsClassifier(),\n",
    "        'hyperparameters': {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        \"name\": 'Logistic Regression',\n",
    "        'estimator':LogisticRegression(),\n",
    "        'hyperparameters': {\n",
    "            \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"], \n",
    "        }\n",
    "        \n",
    "    }, \n",
    "    {\n",
    "        'name': \"Random Forest Classifier\",\n",
    "        'estimator':RandomForestClassifier(), \n",
    "        'hyperparameters': {\n",
    "            \"n_estimators\": [10, 50, 200],\n",
    "            \"criterion\": [\"entropy\", \"gini\"],\n",
    "            \"max_depth\": [2, 5, 10],\n",
    "            \"max_features\": [\"log2\", \"sqrt\"],\n",
    "            \"min_samples_leaf\": [1, 5, 8],\n",
    "            \"min_samples_split\": [2, 3, 5]\n",
    "        }\n",
    "    }]\n",
    "    for model in models:\n",
    "        print(model['name'], \"\\n_____________\\n\")\n",
    "        grid = GridSearchCV(model[\"estimator\"], \n",
    "                            param_grid=model[\"hyperparameters\"], \n",
    "                           cv = 10)\n",
    "        grid.fit(X_train, y_train)\n",
    "        model['best model'] = grid.best_estimator_\n",
    "        model['best parameters'] = grid.best_params_\n",
    "        model['best score'] = grid.best_score_\n",
    "        print('best score:',grid.best_score_)\n",
    "        print('best parameters:', grid.best_params_)\n",
    "        print(\"\\n\\n\")\n",
    "    return models\n",
    "best_models = model_selector(data, best_features)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "rf = RandomForestClassifier(max_depth=2, max_features='log2', min_samples_leaf=5, \n",
    "                            min_samples_split=2, n_estimators=200)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions), accuracy_score(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Tonsillectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_tonsillectomy = data.corr()\n",
    "(corr_tonsillectomy['Tonsillectomy']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tonsil_model = model_selector(data, best_features, 'Pus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[best_features], data['Tonsillectomy'], test_size=0.33, random_state=42)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=10, max_features='log2', min_samples_leaf=20, \n",
    "                            min_samples_split=2, n_estimators=4, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "print(accuracy_train, accuracy)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pus = data[data['Pus']==1]\n",
    "neg_pus = data[data['Pus']==0]\n",
    "samp = pos_pus.sample(n=neg_pus.shape[0])\n",
    "subsampled_data = pd.concat([neg_pus, samp], axis=0)\n",
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
